class AudioVisualGazeModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        # Existing encoders
        self.video_encoder = VideoEncoder(config)
        self.audio_encoder = AudioEncoder(config)
        
        # Convolutional embedding processors (from diagram)
        self.video_spatial_conv = nn.Conv1d(config.video_dim, config.hidden_dim, 1)
        self.audio_spatial_conv = nn.Conv1d(config.audio_dim, config.hidden_dim, 1)
        self.video_temporal_conv = nn.Conv1d(config.video_dim, config.hidden_dim, 1)
        self.audio_temporal_conv = nn.Conv1d(config.audio_dim, config.hidden_dim, 1)
        
        # Attention modules (from diagram)
        self.spatial_attention = InFrameSelfAttention(config)
        self.temporal_attention = CrossFrameSelfAttention(config)
        
        # Decoder and other components
        self.decoder = GazeDecoder(config)
        
        # ADD YOUR RL POLICY NETWORKS HERE
        # 1. Spatial fusion policy
        self.spatial_policy = RLPolicy(
            input_dim=config.video_dim + config.audio_dim,  # Combined embedding dimension
            hidden_dim=256,
            temperature=1.0  # Start with higher temperature
        )
        
        # 2. Temporal fusion policy
        self.temporal_policy = RLPolicy(
            input_dim=config.video_dim + config.audio_dim,
            hidden_dim=256,
            temperature=1.0
        )
        
        # Output projection layers (from diagram)
        self.video_projection = nn.Linear(config.hidden_dim, config.output_dim)
        self.audio_projection = nn.Linear(config.hidden_dim, config.output_dim)
        
    def forward(self, video_frames, audio_specs):
        # Encode inputs
        video_embed = self.video_encoder(video_frames)  # [B, T, W, D_v]
        audio_embed = self.audio_encoder(audio_specs)   # [B, T, M, D_a]
        
        # Process for spatial fusion
        v_spatial = self.video_spatial_conv(video_embed)  # [B, T, W, D_h]
        a_spatial = self.audio_spatial_conv(audio_embed)  # [B, T, M, D_h]
        
        # 1. APPLY SPATIAL POLICY HERE - For each frame
        spatial_fusion_weights = []
        weighted_spatial_embeds = []
        
        for t in range(video_embed.shape[1]):  # For each time step
            # Get mean embeddings for policy input
            v_mean = v_spatial[:, t].mean(dim=1)  # [B, D_h]
            a_mean = a_spatial[:, t].mean(dim=1)  # [B, D_h]
            
            # Get fusion weight from policy
            fusion_weight = self.spatial_policy(v_mean, a_mean)  # [B, 1]
            spatial_fusion_weights.append(fusion_weight)
            
            # Apply weight to spatial embeddings
            v_weighted = fusion_weight.unsqueeze(-1) * v_spatial[:, t]  # [B, W, D_h]
            a_weighted = (1 - fusion_weight).unsqueeze(-1) * a_spatial[:, t]  # [B, M, D_h]
            
            # Store weighted embeddings
            weighted_spatial_embeds.append((v_weighted, a_weighted))
        
        # Process spatial attention with weighted embeddings
        spatial_outputs = []
        for t, (v_weighted, a_weighted) in enumerate(weighted_spatial_embeds):
            # Process through in-frame self-attention
            v_attended = self.spatial_attention(v_weighted, a_weighted)
            spatial_outputs.append(v_attended)
        
        # Stack spatial outputs back to sequence
        u_v_s = torch.stack(spatial_outputs, dim=1)  # [B, T, D_h]
        
        # Process for temporal fusion
        v_temporal = self.video_temporal_conv(video_embed)  # [B, T, W, D_h]
        a_temporal = self.audio_temporal_conv(audio_embed)  # [B, T, M, D_h]
        
        # 2. APPLY TEMPORAL POLICY HERE
        # Get sequence-level mean embeddings
        v_seq_mean = v_temporal.mean(dim=2).mean(dim=1)  # [B, D_h]
        a_seq_mean = a_temporal.mean(dim=2).mean(dim=1)  # [B, D_h]
        
        # Get temporal fusion weight
        temporal_fusion_weight = self.temporal_policy(v_seq_mean, a_seq_mean)  # [B, 1]
        
        # Apply weight to temporal streams
        v_temporal_weighted = temporal_fusion_weight.unsqueeze(1).unsqueeze(2) * v_temporal
        a_temporal_weighted = (1 - temporal_fusion_weight).unsqueeze(1).unsqueeze(2) * a_temporal
        
        # Process through cross-frame self-attention
        u_v_t, u_a_t = self.temporal_attention(v_temporal_weighted, a_temporal_weighted)
        
        # Project outputs
        u_v = self.video_projection(u_v_s + u_v_t)  # Combined video representation
        u_a = self.audio_projection(u_a_t)          # Audio representation
        
        # Decode to gaze predictions
        gaze_preds = self.decoder(u_v, u_a)
        
        # Return predictions and fusion weights for loss computation
        return gaze_preds, video_embed, audio_embed, torch.cat(spatial_fusion_weights, dim=0), temporal_fusion_weight